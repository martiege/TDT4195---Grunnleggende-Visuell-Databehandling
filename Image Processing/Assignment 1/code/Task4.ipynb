{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import tqdm\n",
    "import dataloaders\n",
    "import torchvision\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "class FullyConnectedModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We are using 28x28 greyscale images.\n",
    "        num_input_nodes = 28*28\n",
    "        # Number of classes in the MNIST dataset\n",
    "        num_classes = 10\n",
    "\n",
    "        # Define our model\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_input_nodes, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Runs a forward pass on the images\n",
    "        x = x.view(-1, 28*28)\n",
    "        out = self.classifier(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters & Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.0192 # 1.0 # .0192\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "# Use CrossEntropyLoss for multi-class classification\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Model definition\n",
    "model = FullyConnectedModel()\n",
    "\n",
    "# Define optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=learning_rate)\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "  torchvision.transforms.ToTensor(), \n",
    "  torchvision.transforms.Normalize([0.5], [0.25])\n",
    "])\n",
    "dataloader_train, dataloader_val = dataloaders.load_dataset(batch_size, image_transform=image_transform)\n",
    "\n",
    "# normalized_image_transform = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize([0.5], [0.25])\n",
    "# ])\n",
    "# normalized_dataloader_train, normalized_dataloader_val = dataloaders.load_dataset(batch_size, image_transform=normalized_image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0: 100%|██████████| 938/938 [00:09<00:00, 101.48it/s]\n",
      "Training epoch 1: 100%|██████████| 938/938 [00:09<00:00, 102.31it/s]\n",
      "Training epoch 2: 100%|██████████| 938/938 [00:09<00:00, 103.31it/s]\n",
      "Training epoch 3: 100%|██████████| 938/938 [00:09<00:00, 104.13it/s]\n",
      "Training epoch 4: 100%|██████████| 938/938 [00:09<00:00, 103.56it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  dataloader_train=dataloader_train,\n",
    "  dataloader_val=dataloader_val,\n",
    "  batch_size=batch_size,\n",
    "  loss_function=loss_function,\n",
    "  optimizer=optimizer\n",
    ")\n",
    "train_loss_dict, val_loss_dict = trainer.train(num_epochs)\n",
    "\n",
    "\n",
    "# normalized_trainer = Trainer(\n",
    "#   model=model,\n",
    "#   dataloader_train=normalized_dataloader_train,\n",
    "#   dataloader_val=normalized_dataloader_val,\n",
    "#   batch_size=batch_size,\n",
    "#  loss_function=loss_function, \n",
    "#   optimizer=optimizer\n",
    "# )\n",
    "# normalized_train_loss_dict, normalized_val_loss_dict = normalized_trainer.train(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = next(model.classifier.children()).weight.data\n",
    "\n",
    "for w in range(weight.shape[0]):\n",
    "    im = np.zeros((28, 28))\n",
    "    \n",
    "    minimum = weight[w, :].min()\n",
    "    maximum = weight[w, :].max()\n",
    "    \n",
    "    for y in range(28):\n",
    "        for x in range(28):\n",
    "            im[y, x] = float((weight[w, y * 28 + x] - minimum) / (maximum - minimum))\n",
    "    \n",
    "    plt.imsave(\"weights/class_\" + str(w) + \"_weight_image.jpg\", im, cmap=\"gray\")\n",
    "    # print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot loss\n",
    "utils.plot_loss(train_loss_dict, label=\"Train Loss\")\n",
    "utils.plot_loss(val_loss_dict, label=\"Test Loss\")\n",
    "# plt.ylim([0, 1])\n",
    "plt.ylim([0, 100])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Images Seen\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.savefig(\"new_learning_training_loss.png\")\n",
    "\n",
    "plt.show()\n",
    "torch.save(model.state_dict(), \"new_learning_saved_model.torch\")\n",
    "final_loss, final_acc = utils.compute_loss_and_accuracy(\n",
    "    dataloader_val, model, loss_function)\n",
    "print(f\"Final Test Cross Entropy Loss: {final_loss}. Final Test accuracy: {final_acc}\")\n",
    "\n",
    "# Plot loss\n",
    "# utils.plot_loss(normalized_train_loss_dict, label=\"Train Loss\")\n",
    "# utils.plot_loss(normalized_val_loss_dict, label=\"Test Loss\")\n",
    "# plt.ylim([0, 1])\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"Number of Images Seen\")\n",
    "# plt.ylabel(\"Cross Entropy Loss\")\n",
    "# plt.savefig(\"normalized_training_loss.png\")\n",
    "\n",
    "# plt.show()\n",
    "# torch.save(model.state_dict(), \"normalized_saved_model.torch\")\n",
    "# final_loss, final_acc = utils.compute_loss_and_accuracy(\n",
    "#     normalized_dataloader_val, model, loss_function)\n",
    "# print(f\"Final Test Cross Entropy Loss: {final_loss}. Final Test accuracy: {final_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(val_loss_dict), min(val_loss_dict))\n",
    "print(max(train_loss_dict), min(train_loss_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
